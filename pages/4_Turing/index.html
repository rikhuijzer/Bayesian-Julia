<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/Bayesian-Julia/libs/katex/katex.min.css"> <link rel=stylesheet  href="/Bayesian-Julia/libs/highlight/github.min.css"> <link rel=stylesheet  href="/Bayesian-Julia/css/jtd.css"> <link rel=icon  href="/Bayesian-Julia/assets/favicon.ico"> <title>How to use Turing</title> <div class=page-wrap > <div class=side-bar > <div class=header > <a href="/Bayesian-Julia/" class=title > Bayesian Stats </a> </div> <label for=show-menu  class=show-menu >MENU</label> <input type=checkbox  id=show-menu  role=button > <div class=menu  id=side-menu > <ul class=menu-list > <li class="menu-list-item "><a href="/Bayesian-Julia/" class="menu-list-link ">Home</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/1_why_Julia/" class="menu-list-link ">1. Why Julia?</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/2_bayes_stats/" class="menu-list-link ">2. What is Bayesian Statistics?</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/3_prob_dist/" class="menu-list-link ">3. Common Probability Distributions</a> <li class="menu-list-item active"><a href="/Bayesian-Julia/pages/4_Turing/" class="menu-list-link active">4. How to use Turing</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/5_MCMC/" class="menu-list-link ">5. Markov Chain Monte Carlo (MCMC)</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/6_linear_reg/" class="menu-list-link ">6. Bayesian Linear Regression</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/7_logistic_reg/" class="menu-list-link ">7. Bayesian Logistic Regression</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/8_count_reg/" class="menu-list-link ">8. Bayesian Regression with Count Data</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/9_robust_reg/" class="menu-list-link ">9. Robust Bayesian Regression</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/10_multilevel_models/" class="menu-list-link ">10. Multilevel Models</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/11_Turing_tricks/" class="menu-list-link ">11. Computational Tricks with Turing</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/12_epi_models/" class="menu-list-link ">12. Bayesian Epidemiological Models</a> </ul> </div> <div class=footer > <a href="https://www.julialang.org"><img style="height:50px;padding-left:10px;margin-bottom:15px;" src="https://julialang.org/assets/infra/logo.svg" alt="Julia Logo"></a> </div> </div> <div class=main-content-wrap > <div class=main-content > <div class=main-header > <a id=github  href="https://github.com/storopoli/Bayesian-Julia">Code on GitHub</a> </div> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-186284914-6"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-186284914-6'); </script> <div class=franklin-content ><div class=franklin-toc ><ol><li><a href="#probabilistic_programming">Probabilistic Programming</a><li><a href="#turings_ecosystem">Turing&#39;s Ecosystem</a><li><a href="#turingjl"><code>Turing.jl</code></a><ol><li><a href="#simulating_data">Simulating Data</a><li><a href="#visualizations">Visualizations</a></ol><li><a href="#prior_and_posterior_predictive_checks">Prior and Posterior Predictive Checks</a><li><a href="#conclusion">Conclusion</a><li><a href="#footnotes">Footnotes</a><li><a href="#references">References</a></ol></div> <h1 id=how_to_use_turing ><a href="#how_to_use_turing" class=header-anchor >How to use Turing</a></h1> <p><a href="http://turing.ml/"><strong>Turing</strong></a> is an ecosystem of Julia packages for Bayesian Inference using <a href="https://en.wikipedia.org/wiki/Probabilistic_programming">probabilistic programming</a>. Turing provides an easy and intuitive way of specifying models.</p> <h2 id=probabilistic_programming ><a href="#probabilistic_programming" class=header-anchor >Probabilistic Programming</a></h2> <p>What is <strong>probabilistic programming</strong> &#40;PP&#41;? It is a <strong>programming paradigm</strong> in which probabilistic models are specified and inference for these models is performed <strong>automatically</strong> &#40;Hardesty, 2015&#41;. In more clear terms, PP and PP Languages &#40;PPLs&#41; allows us to specify <strong>variables as random variables</strong> &#40;like Normal, Binominal etc.&#41; with <strong>known or unknown parameters</strong>. Then, we <strong>construct a model</strong> using these variables by specifying how the variables related to each other, and finally <strong>automatic inference of the variables&#39; unknown parameters</strong> is then performed.</p> <p>In a Bayesian approach this means specifying <strong>priors</strong>, <strong>likelihoods</strong> and letting the PPL compute the <strong>posterior</strong>. Since the denominator in the posterior is often intractable, we use Markov Chain Monte Carlo<sup id="fnref:MCMC"><a href="#fndef:MCMC" class=fnref >[1]</a></sup> and some fancy algorithm that uses the posterior geometry to guide the MCMC proposal using Hamiltonian dynamics called Hamiltonian Monte Carlo &#40;HMC&#41; to approximate the posterior. This involves, besides a suitable PPL, automatic differentiation, MCMC chains interface, and also an efficient HMC algorithm implementation. In order to provide all of these features, Turing has a whole ecosystem to address each and every one of these components.</p> <h2 id=turings_ecosystem ><a href="#turings_ecosystem" class=header-anchor >Turing&#39;s Ecosystem</a></h2> <p>Before we dive into how to specify models in Turing, let&#39;s discuss Turing&#39;s <strong>ecosystem</strong>. We have several Julia packages under Turing&#39;s GitHub organization <a href="https://github.com/TuringLang">TuringLang</a>, but I will focus on 6 of those:</p> <ul> <li><p><a href="https://github.com/TuringLang/Turing.jl"><code>Turing.jl</code></a></p> <li><p><a href="https://github.com/TuringLang/MCMCChains.jl"><code>MCMCChains.jl</code></a></p> <li><p><a href="https://github.com/TuringLang/DynamicPPL.jl"><code>DynamicPPL.jl</code></a></p> <li><p><a href="https://github.com/TuringLang/AdvancedHMC.jl"><code>AdvancedHMC.jl</code></a></p> <li><p><a href="https://github.com/TuringLang/DistributionsAD.jl"><code>DistributionsAD.jl</code></a></p> <li><p><a href="https://github.com/TuringLang/Bijectors.jl"><code>Bijectors.jl</code></a></p> </ul> <p>The first one is <a href="https://github.com/TuringLang/Turing.jl"><code>Turing.jl</code></a> &#40;Ge, Xu &amp; Ghahramani, 2018&#41; itself, the main package that we use to <strong>interface with all the Turing ecosystem</strong> of packages and the backbone of the PPL Turing.</p> <p>The second, <a href="https://github.com/TuringLang/MCMCChains.jl"><code>MCMCChains.jl</code></a>, is an interface to <strong>summarizing MCMC simulations</strong> and has several utility functions for <strong>diagnostics</strong> and <strong>visualizations</strong>.</p> <p>The third package is <a href="https://github.com/TuringLang/DynamicPPL.jl"><code>DynamicPPL.jl</code></a> &#40;Tarek, Xu, Trapp, Ge &amp; Ghahramani, 2020&#41; which specifies a domain-specific language and backend for Turing &#40;which itself is a PPL&#41;. The main feature of <code>DynamicPPL.jl</code> is that is is entirely written in Julia and also it is modular.</p> <p><a href="https://github.com/TuringLang/AdvancedHMC.jl"><code>AdvancedHMC.jl</code></a> &#40;Xu, Ge, Tebbutt, Tarek, Trapp &amp; Ghahramani, 2020&#41; provides a robust, modular and efficient implementation of advanced HMC algorithms. The state-of-the-art HMC algorithm is the <strong>N</strong>o-<strong>U</strong>-<strong>T</strong>urn <strong>S</strong>ampling &#40;NUTS&#41;<sup id="fnref:MCMC"><a href="#fndef:MCMC" class=fnref >[1]</a></sup> &#40;Hoffman &amp; Gelman, 2011&#41; which is available in <code>AdvancedHMC.jl</code>.</p> <p>The fourth package, <a href="https://github.com/TuringLang/DistributionsAD.jl"><code>DistributionsAD.jl</code></a> defines the necessary functions to enable automatic differentiation &#40;AD&#41; of the <code>logpdf</code> function from <a href="https://github.com/JuliaStats/Distributions.jl"><code>Distributions.jl</code></a> using the packages <a href="https://github.com/FluxML/Tracker.jl"><code>Tracker.jl</code></a>, <a href="https://github.com/FluxML/Zygote.jl"><code>Zygote.jl</code></a>, <a href="https://github.com/JuliaDiff/ForwardDiff.jl"><code>ForwardDiff.jl</code></a> and <a href="https://github.com/JuliaDiff/ReverseDiff.jl"><code>ReverseDiff.jl</code></a>. The main goal of <code>DistributionsAD.jl</code> is to make the output of <code>logpdf</code> differentiable with respect to all continuous parameters of a distribution as well as the random variable in the case of continuous distributions. This is the package that guarantees the &quot;automatic inference&quot; part of the definition of a PPL.</p> <p>Finally, <a href="https://github.com/TuringLang/Bijectors.jl"><code>Bijectors.jl</code></a> implements a set of functions for transforming constrained random variables &#40;e.g. simplexes, intervals&#41; to Euclidean space. Note that <code>Bijectors.jl</code> is still a work-in-progress and in the future we&#39;ll have better implementation for more constraints, <em>e.g.</em> positive ordered vectors of random variables.</p> <p>Most of the time we will not be dealing with these packages directly, since <code>Turing.jl</code> will take care of the interfacing for us. So let&#39;s talk about <code>Turing.jl</code>.</p> <h2 id=turingjl ><a href="#turingjl" class=header-anchor ><code>Turing.jl</code></a></h2> <p><code>Turing.jl</code> is the main package in the Turing ecosystem and the backbone that glues all the other packages together. Turing&#39;s &quot;workflow&quot; begin with a model specification. We specify the model inside a macro <code>@model</code> where we can assign variables in two ways:</p> <ul> <li><p>using <code>~</code>: which means that a variable follows some probability distribution &#40;Normal, Binomial etc.&#41; and its value is random under that distribution</p> <li><p>using <code>&#61;</code>: which means that a variable does not follow a probability distribution and its value is deterministic &#40;like the normal <code>&#61;</code> assignment in programming languages&#41;</p> </ul> <p>Turing will perform automatic inference on all variables that you specify using <code>~</code>. Here is a simple example of how we would model a six-sided dice. Note that a &quot;fair&quot; dice will be distributed as a discrete uniform probability with the lower bound as 1 and the upper bound as 6:</p> <a id=uniformdice  class=anchor ></a>\[ X \sim \text{Uniform}(1,6) \] <p>Note that the expectation of a random variable \(X \sim \text{Uniform}(a,b)\) is:</p> <a id=expectationdice  class=anchor ></a>\[ E(X) = \frac{a+b}{2} = \frac{7}{2} = 3.5 \] <p>Graphically this means:</p> <pre><code class=language-julia >using Plots, StatsPlots, Distributions, LaTeXStrings

dice &#61; DiscreteUniform&#40;1, 6&#41;
plot&#40;dice,
    label&#61;&quot;six-sided Dice&quot;,
    markershape&#61;:circle,
    ms&#61;5,
    xlabel&#61;L&quot;\theta&quot;,
    ylabel&#61;&quot;Mass&quot;,
    ylims&#61;&#40;0, 0.3&#41;
&#41;
vline&#33;&#40;&#91;mean&#40;dice&#41;&#93;, lw&#61;5, col&#61;:red, label&#61;L&quot;E&#40;\theta&#41;&quot;&#41;</code></pre> <p><img src="/Bayesian-Julia/assets/pages/4_Turing/code/output/dice.svg" alt=""> <div class=text-center ><em>A &quot;fair&quot; six-sided Dice: Discrete Uniform between 1 and 6</em></div> <br/></p> <p>So let&#39;s specify our first Turing model. It will be named <code>dice_throw</code> and will have a single parameter <code>y</code> which is a \(N\)-dimensional vector of integers representing the observed data, <em>i.e.</em> the outcomes of \(N\) six-sided dice throws:</p> <pre><code class=language-julia >using Turing

@model dice_throw&#40;y&#41; &#61; begin
    #Our prior belief about the probability of each result in a six-sided dice.
    #p is a vector of length 6 each with probability p that sums up to 1.
    p ~ Dirichlet&#40;6, 1&#41;

    #Each outcome of the six-sided dice has a probability p.
    y ~ filldist&#40;Categorical&#40;p&#41;, length&#40;y&#41;&#41;
end;</code></pre> <p>Here we are using the <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distribution</a> which is the multivariate generalization of the <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>. The Dirichlet distribution is often used as the conjugate prior for Categorical or Multinomial distributions. Our dice is modelled as a <a href="https://en.wikipedia.org/wiki/Categorical_distribution">Categorical distribution</a> with six possible results \(y \in \{ 1, 2, 3, 4, 5, 6 \}\) with some probability vector \(\mathbf{p} = (p_1, \dots, p_6)\). Since all mutually exclusive outcomes must sum up to 1 to be a valid probability, we impose the constraint that all \(p\)s must sum up to 1 – \(\sum^n_{i=1} p_i = 1\). We could have used a vector of six Beta random variables but it would be hard and inefficient to enforce this constraint. Instead, I&#39;ve opted for a Dirichlet with a weekly informative prior towards a &quot;fair&quot; dice which is encoded as a <code>Dirichlet&#40;6,1&#41;</code>. This is translated as a 6-dimensional vector of elements that sum to one:</p> <pre><code class=language-julia >mean&#40;Dirichlet&#40;6, 1&#41;&#41;</code></pre><pre><code class="plaintext code-output">6-element Fill{Float64}: entries equal to 0.16666666666666666</code></pre>
<p>And, indeed, it sums up to one:</p>
<pre><code class=language-julia >sum&#40;mean&#40;Dirichlet&#40;6, 1&#41;&#41;&#41;</code></pre><pre><code class="plaintext code-output">1.0</code></pre>
<p>Also, since the outcome of a <a href="https://en.wikipedia.org/wiki/Categorical_distribution">Categorical distribution</a> is an integer and <code>y</code> is a \(N\)-dimensional vector of integers we need to apply some sort of broadcasting here. <code>filldist&#40;&#41;</code> is a nice Turing function which takes any univariate or multivariate distribution and returns another distribution that repeats the input distribution. We could also use the familiar dot <code>.</code> broadcasting operator in Julia: <code>y .~ Categorical&#40;p&#41;</code> to signal that all elements of <code>y</code> are distributed as a Categorical distribution. But doing that does not allow us to do predictive checks &#40;more on this below&#41;. So, instead we use <code>filldist&#40;&#41;</code>.</p>
<h3 id=simulating_data ><a href="#simulating_data" class=header-anchor >Simulating Data</a></h3>
<p>Now let&#39;s set a seed for the pseudo-random number generator and simulate 1,000 throws of a six-sided dice:</p>
<pre><code class=language-julia >using Random

Random.seed&#33;&#40;123&#41;;

data &#61; rand&#40;DiscreteUniform&#40;1, 6&#41;, 1_000&#41;;</code></pre>
<p>The vector <code>data</code> is a 1,000-length vector of <code>Int</code>s ranging from 1 to 6, just like how a regular six-sided dice outcome would be:</p>
<pre><code class=language-julia >first&#40;data, 5&#41;</code></pre><pre><code class="plaintext code-output">5-element Vector{Int64}:
 6
 3
 2
 3
 4</code></pre>
<p>Once the model is specified we instantiate the model with the single parameter <code>y</code> as the simulated <code>data</code>:</p>
<pre><code class=language-julia >model &#61; dice_throw&#40;data&#41;;</code></pre>
<p>Next, we call Turing&#39;s <code>sample&#40;&#41;</code> function that takes a Turing model as a first argument, along with a sampler as the second argument, and the third argument is the number of iterations. Here, I will use the <code>NUTS&#40;&#41;</code> sampler from <code>AdvancedHMC.jl</code> and 2,000 iterations. Please note that, as default, Turing samplers will discard the first half of iterations as warmup. So the sampler will output 1,000 samples &#40;<code>floor&#40;2_000 / 2&#41;</code>&#41;:</p>
<pre><code class=language-julia >chain &#61; sample&#40;model, NUTS&#40;&#41;, 2_000&#41;;</code></pre>
<p>Now let&#39;s inspect the chain. We can do that with the function <code>describe&#40;&#41;</code> that will return a 2-element vector of <code>ChainDataFrame</code> &#40;this is the type defined by <code>MCMCChains.jl</code> to store Markov chain&#39;s information regarding the inferred parameters&#41;. The first <code>ChainDataFrame</code> has information regarding the parameters&#39; summary statistics &#40;<code>mean</code>, <code>std</code>, <code>r_hat</code>, ...&#41; and the second is the parameters&#39; quantiles. Since <code>describe&#40;chain&#41;</code> returns a 2-element vector, I will assign the output to two variables:</p>
<pre><code class=language-julia >summaries, quantiles &#61; describe&#40;chain&#41;;</code></pre>
<p>We won&#39;t be focusing on quantiles, so let&#39;s put it aside for now. Let&#39;s then take a look at the parameters&#39; summary statistics:</p>
<pre><code class=language-julia >summaries</code></pre><pre><code class="plaintext code-output">Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64

        p[1]    0.1571    0.0116     0.0003    0.0001   2627.3417    0.9996       66.9898
        p[2]    0.1823    0.0120     0.0003    0.0003   2757.7237    0.9998       70.3142
        p[3]    0.1417    0.0111     0.0002    0.0002   3062.0785    1.0004       78.0744
        p[4]    0.1871    0.0122     0.0003    0.0002   2753.3811    0.9995       70.2035
        p[5]    0.1622    0.0116     0.0003    0.0002   2669.2283    0.9999       68.0578
        p[6]    0.1696    0.0122     0.0003    0.0003   2787.7379    0.9995       71.0795
</code></pre>
<p>Here <code>p</code> is a 6-dimensional vector of probabilities, which each one associated with a mutually exclusive outcome of a six-sided dice throw. As we expected, the probabilities are almost equal to \(\frac{1}{6}\), like a &quot;fair&quot; six-sided dice that we simulated data from &#40;sampling from <code>DiscreteUniform&#40;1, 6&#41;</code>&#41;. Indeed, just for a sanity check, the mean of the estimates of <code>p</code> sums up to 1:</p>
<pre><code class=language-julia >sum&#40;summaries&#91;:, :mean&#93;&#41;</code></pre><pre><code class="plaintext code-output">1.0</code></pre>
<p>In the future if you have some crazy huge models and you just want a <strong>subset</strong> of parameters from your chains? Just do <code>group&#40;chain, :parameter&#41;</code> or index with <code>chain&#91;:, 1:6, :&#93;</code>:</p>
<pre><code class=language-julia >summarystats&#40;chain&#91;:, 1:3, :&#93;&#41;</code></pre><pre><code class="plaintext code-output">Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64

        p[1]    0.1571    0.0116     0.0003    0.0001   2627.3417    0.9996       66.9898
        p[2]    0.1823    0.0120     0.0003    0.0003   2757.7237    0.9998       70.3142
        p[3]    0.1417    0.0111     0.0002    0.0002   3062.0785    1.0004       78.0744
</code></pre>
<p>or <code>chain&#91;&#91;:parameters,...&#93;&#93;</code>:</p>
<pre><code class=language-julia >summarystats&#40;chain&#91;&#91;:var&quot;p&#91;1&#93;&quot;, :var&quot;p&#91;2&#93;&quot;&#93;&#93;&#41;</code></pre><pre><code class="plaintext code-output">Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64

        p[1]    0.1571    0.0116     0.0003    0.0001   2627.3417    0.9996       66.9898
        p[2]    0.1823    0.0120     0.0003    0.0003   2757.7237    0.9998       70.3142
</code></pre>
<p>And, finally let&#39;s compute the expectation of the estimated six-sided dice, \(E(\tilde{X})\), using the standard expectation definition of expectation for a discrete random variable:</p>
\[ E(X) = \sum_{x \in X} x \cdot P(x) \]
<pre><code class=language-julia >sum&#40;&#91;idx * i for &#40;i, idx&#41; in enumerate&#40;summaries&#91;:, :mean&#93;&#41;&#93;&#41;</code></pre><pre><code class="plaintext code-output">3.523564797218474</code></pre>
<p>Bingo&#33; The estimated expectation is very <em>close</em> to the theoretical expectation of \(\frac{7}{2} = 3.5\), as we&#39;ve show in <span class=eqref >(<a href="#expectationdice">2</a>)</span>.</p>
<h3 id=visualizations ><a href="#visualizations" class=header-anchor >Visualizations</a></h3>
<p>Note that the type of our <code>chain</code> is a <code>Chains</code> object from <code>MCMCChains.jl</code>:</p>
<pre><code class=language-julia >typeof&#40;chain&#41;</code></pre><pre><code class="plaintext code-output">MCMCChains.Chains{Float64, AxisArrays.AxisArray{Float64, 3, Array{Float64, 3}, Tuple{AxisArrays.Axis{:iter, StepRange{Int64, Int64}}, AxisArrays.Axis{:var, Vector{Symbol}}, AxisArrays.Axis{:chain, UnitRange{Int64}}}}, Missing, NamedTuple{(:parameters, :internals), Tuple{Vector{Symbol}, Vector{Symbol}}}, NamedTuple{(:start_time, :stop_time), Tuple{Float64, Float64}}}</code></pre>
<p>We can use plotting capabilities of <code>MCMCChains.jl</code> with any <code>Chains</code> object:</p>
<pre><code class=language-julia >plot&#40;chain&#41;</code></pre>
<p><img src="/Bayesian-Julia/assets/pages/4_Turing/code/output/chain.svg" alt=""> <div class=text-center ><em>Visualization of a MCMC Chain simulation</em></div> <br/></p>
<p>On the figure above we can see, for each parameter in the model, on the left the parameter&#39;s traceplot and on the right the parameter&#39;s density<sup id="fnref:visualization"><a href="#fndef:visualization" class=fnref >[2]</a></sup>.</p>
<h2 id=prior_and_posterior_predictive_checks ><a href="#prior_and_posterior_predictive_checks" class=header-anchor >Prior and Posterior Predictive Checks</a></h2>
<p>Predictive checks are a great way to <strong>validate a model</strong>. The idea is to <strong>generate data from the model</strong> using <strong>parameters from draws from the prior or posterior</strong>. <strong>Prior predictive check</strong> is when we simulate data using model parameter values drawn fom the <strong>prior</strong> distribution, and <strong>posterior predictive check</strong> is is when we simulate data using model parameter values drawn fom the <strong>posterior</strong> distribution.</p>
<p>The workflow we do when specifying and sampling Bayesian models is not linear or acyclic &#40;Gelman et al., 2020&#41;. This means that we need to iterate several times between the different stages in order to find a model that captures best the data generating process with the desired assumptions. The figure below demonstrates the workflow <sup id="fnref:workflow"><a href="#fndef:workflow" class=fnref >[3]</a></sup>.</p>
<p><img src="/Bayesian-Julia/pages/images/bayesian_workflow.png" alt="Bayesian Workflow" /></p>
<p><div class=text-center ><em>Bayesian Workflow. Adapted from Gelman et al. &#40;2020&#41;</em></div> <br/></p>
<p>This is quite easy in Turing. Our six-sided dice model already has a <strong>posterior distribution</strong> which is the object <code>chain</code>. We need to create a <strong>prior distribution</strong> for our model. To accomplish this, instead of supplying a MCMC sampler like <code>NUTS&#40;&#41;</code>, we supply the &quot;sampler&quot; <code>Prior&#40;&#41;</code> inside Turing&#39;s <code>sample&#40;&#41;</code> function:</p>
<pre><code class=language-julia >prior_chain &#61; sample&#40;model, Prior&#40;&#41;, 2_000&#41;;</code></pre>
<p>Now we can perform predictive checks using both the prior &#40;<code>prior_chain</code>&#41; or posterior &#40;<code>chain</code>&#41; distributions. To draw from the prior and posterior predictive distributions we instantiate a &quot;predictive model&quot;, <em>i.e.</em> a Turing model but with the observations set to <code>missing</code><sup id="fnref:missing"><a href="#fndef:missing" class=fnref >[4]</a></sup>, and then calling <code>predict&#40;&#41;</code> on the predictive model and the previously drawn samples. First let&#39;s do the <em>prior</em> predictive check:</p>
<pre><code class=language-julia >missing_data &#61; Vector&#123;Missing&#125;&#40;missing, length&#40;data&#41;&#41; # vector of &#96;missing&#96;
model_missing &#61; dice_throw&#40;missing_data&#41;
model_predict &#61; DynamicPPL.Model&#123;&#40;:y,&#41;&#125;&#40;:model_predict_missing_data,
                    model_missing.f,
                    model_missing.args,
                    model_missing.defaults&#41; # instantiate the &quot;predictive model&quot;
prior_check &#61; predict&#40;model_predict, prior_chain&#41;;</code></pre>
<p>Here we are creating a <code>missing_data</code> object which is a <code>Vector</code> of the same length as the <code>data</code> and populated with type <code>missing</code> as values. We then instantiate a new <code>dice_throw</code> model with the <code>missing_data</code> vector as the <code>data</code> argument. We proceed by instantiating a new Turing <code>DynamicPPL.Model</code> model with the <code>missing_data</code> vector as the <code>data</code> argument. The boilerplate around <code>DynamicPPL.Model</code> is the default arguments that a <code>DynamicPPL.Model</code> model needs to have. Finally, we call <code>predict&#40;&#41;</code> on the predictive model and the previously drawn samples, which in our case are the samples from the prior distribution &#40;<code>prior_chain</code>&#41;.</p>
<p>Note that <code>predict&#40;&#41;</code> returns a <code>Chains</code> object from <code>MCMCChains.jl</code>:</p>
<pre><code class=language-julia >typeof&#40;prior_check&#41;</code></pre><pre><code class="plaintext code-output">MCMCChains.Chains{Float64, AxisArrays.AxisArray{Float64, 3, Array{Float64, 3}, Tuple{AxisArrays.Axis{:iter, StepRange{Int64, Int64}}, AxisArrays.Axis{:var, Vector{Symbol}}, AxisArrays.Axis{:chain, UnitRange{Int64}}}}, Missing, NamedTuple{(:parameters, :internals), Tuple{Vector{Symbol}, Vector{Symbol}}}, NamedTuple{(), Tuple{}}}</code></pre>
<p>And we can call <code>summarystats&#40;&#41;</code>:</p>
<pre><code class=language-julia >summarystats&#40;prior_check&#91;:, 1:5, :&#93;&#41; # just the first 5 prior samples</code></pre><pre><code class="plaintext code-output">Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64

        y[1]    3.5365    1.7141     0.0383    0.0331   1851.0818    0.9996
        y[2]    3.4895    1.6761     0.0375    0.0311   1881.0429    0.9997
        y[3]    3.4810    1.7260     0.0386    0.0459   1690.5725    1.0010
        y[4]    3.5175    1.7124     0.0383    0.0374   2085.2764    1.0006
        y[5]    3.4685    1.6963     0.0379    0.0289   1858.5119    0.9996
</code></pre>
<p>We can do the same with <code>chain</code> for a <em>posterior</em> predictive check:</p>
<pre><code class=language-julia >posterior_check &#61; predict&#40;model_predict, chain&#41;;
summarystats&#40;posterior_check&#91;:, 1:5, :&#93;&#41; # just the first 5 posterior samples</code></pre><pre><code class="plaintext code-output">Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64

        y[1]    3.5045    1.7039     0.0381    0.0326   1886.5475    0.9996
        y[2]    3.4960    1.7226     0.0385    0.0350   2019.5831    0.9997
        y[3]    3.5430    1.6981     0.0380    0.0346   1935.2569    0.9998
        y[4]    3.5290    1.6999     0.0380    0.0389   1974.7849    0.9997
        y[5]    3.4975    1.7183     0.0384    0.0351   2142.0478    1.0003
</code></pre>
<h2 id=conclusion ><a href="#conclusion" class=header-anchor >Conclusion</a></h2>
<p>This is the basic overview of Turing usage. I hope that I could show you how simple and intuitive is to specify probabilistic models using Turing. First, specify a <strong>model</strong> with the macro <code>@model</code>, then <strong>sample from it</strong> by specifying the <strong>data</strong>, <strong>sampler</strong> and <strong>number of interactions</strong>. All <strong>probabilistic parameters</strong> &#40;the ones that you&#39;ve specified using <code>~</code>&#41; will be <strong>inferred</strong> with a full <strong>posterior density</strong>. Finally, you inspect the <strong>parameters&#39; statistics</strong> like <strong>mean</strong> and <strong>standard deviation</strong>, along with <strong>convergence diagnostics</strong> like <code>r_hat</code>. Conveniently, you can <strong>plot</strong> stuff easily if you want to. You can also do <strong>predictive checks</strong> using either the <strong>posterior</strong> or <strong>prior</strong> model&#39;s distributions.</p>
<h2 id=footnotes ><a href="#footnotes" class=header-anchor >Footnotes</a></h2>
<p><table class=fndef  id="fndef:MCMC">
    <tr>
        <td class=fndef-backref ><a href="#fnref:MCMC">[1]</a>
        <td class=fndef-content >see <a href="/Bayesian-Julia/pages/5_MCMC/">5. <strong>Markov Chain Monte Carlo &#40;MCMC&#41;</strong></a>.
    
</table>
<table class=fndef  id="fndef:visualization">
    <tr>
        <td class=fndef-backref ><a href="#fnref:visualization">[2]</a>
        <td class=fndef-content >we&#39;ll cover those plots and diagnostics in <a href="/Bayesian-Julia/pages/5_MCMC/">5. <strong>Markov Chain Monte Carlo &#40;MCMC&#41;</strong></a>.
    
</table>
<table class=fndef  id="fndef:workflow">
    <tr>
        <td class=fndef-backref ><a href="#fnref:workflow">[3]</a>
        <td class=fndef-content >note that this workflow is a extremely simplified adaptation from the original workflow on which it was based. I suggest the reader to consult the original workflow of Gelman et al. &#40;2020&#41;.
    
</table>
<table class=fndef  id="fndef:missing">
    <tr>
        <td class=fndef-backref ><a href="#fnref:missing">[4]</a>
        <td class=fndef-content >in a real-world scenario, you&#39;ll probably want to use more than just <strong>one</strong> observation as a predictive check, so you should use something like <code>Vector&#123;Missing&#125;&#40;missing, length&#40;y&#41;&#41;</code> or <code>fill&#40;missing, length&#40;y&#41;</code>.
    
</table>
</p>
<h2 id=references ><a href="#references" class=header-anchor >References</a></h2>
<p>Ge, H., Xu, K., &amp; Ghahramani, Z. &#40;2018&#41;. Turing: A Language for Flexible Probabilistic Inference. International Conference on Artificial Intelligence and Statistics, 1682–1690. http://proceedings.mlr.press/v84/ge18b.html</p>
<p>Gelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., … Modr’ak, M. &#40;2020, November 3&#41;. Bayesian Workflow. Retrieved February 4, 2021, from http://arxiv.org/abs/2011.01808</p>
<p>Hardesty &#40;2015&#41;.  &quot;Probabilistic programming does in 50 lines of code what used to take thousands&quot;. phys.org. April 13, 2015. Retrieved April 13, 2015. https://phys.org/news/2015-04-probabilistic-lines-code-thousands.html</p>
<p>Hoffman, M. D., &amp; Gelman, A. &#40;2011&#41;. The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15&#40;1&#41;, 1593–1623. Retrieved from http://arxiv.org/abs/1111.4246</p>
<p>Tarek, M., Xu, K., Trapp, M., Ge, H., &amp; Ghahramani, Z. &#40;2020&#41;. DynamicPPL: Stan-like Speed for Dynamic Probabilistic Models. ArXiv:2002.02702 &#91;Cs, Stat&#93;. http://arxiv.org/abs/2002.02702</p>
<p>Xu, K., Ge, H., Tebbutt, W., Tarek, M., Trapp, M., &amp; Ghahramani, Z. &#40;2020&#41;. AdvancedHMC.jl: A robust, modular and efficient implementation of advanced HMC algorithms. Symposium on Advances in Approximate Bayesian Inference, 1–10. http://proceedings.mlr.press/v118/xu20a.html</p>

<div class=page-foot >
  <div class=copyright >
    Last modified: August 12, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div>
    </div> 
    </div> 
    </div> <!-- end of class page-wrap-->
    
      <script src="/Bayesian-Julia/libs/katex/katex.min.js"></script>
<script src="/Bayesian-Julia/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
      <script src="/Bayesian-Julia/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>